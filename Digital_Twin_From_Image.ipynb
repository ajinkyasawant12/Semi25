{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f483cbd3-64c7-4f4a-8b06-0fdfca4d3f33",
   "metadata": {},
   "source": [
    "# Digital Twin from Image using MiDaS, MediaPipe, Open3D, and Plotly\n",
    "\n",
    "This notebook processes a single image (or a frame extracted from a video) to create a 3D digital twin. It performs the following steps:\n",
    "\n",
    "1. **Depth Estimation** using MiDaS to generate a relative depth map.\n",
    "2. **Pose Estimation** using MediaPipe to extract 2D skeleton landmarks.\n",
    "3. **Point Cloud Generation** using Open3D from the RGB-D data.\n",
    "4. **Visualization** using Plotly to show (a) the raw point cloud and (b) the digital twin (point cloud with overlaid skeleton).\n",
    "\n",
    "Upload your test image (e.g. `input.jpg`) to the Colab file system and run the cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784a88b-2bb7-4142-b0af-7e3f85748a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y libosmesa6-dev libgl1-mesa-glx libglfw3\n",
    "\n",
    "!pip install opencv-python-headless mediapipe open3d torch torchvision plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808f82c-1e97-4b6e-8f0f-df0d75c4435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import open3d as o3d\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Enable cuDNN benchmarking for optimal performance\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a32a5-cd3c-44d5-b8dd-986bdc27a7ad",
   "metadata": {},
   "source": [
    "## Load MiDaS Model\n",
    "\n",
    "We use the small MiDaS model for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e85b1-34ef-411f-8e3e-9cb3f5e55af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"MiDaS_small\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.small_transform if model_type == \"MiDaS_small\" else midas_transforms.default_transform\n",
    "print(\"MiDaS model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1072d8-d927-4ef1-b3f6-d5f4f92dd657",
   "metadata": {},
   "source": [
    "## Setup MediaPipe Pose\n",
    "\n",
    "We configure MediaPipe in static image mode for single-image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2d5b8-0a64-4e3e-9530-218f4dd5b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose_estimator = mp_pose.Pose(static_image_mode=True,\n",
    "                              model_complexity=1,\n",
    "                              min_detection_confidence=0.5,\n",
    "                              min_tracking_confidence=0.5)\n",
    "pose_connections = mp_pose.POSE_CONNECTIONS\n",
    "print(\"MediaPipe Pose loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84e008-f52a-4c91-a8fe-88edc6bb73d2",
   "metadata": {},
   "source": [
    "## Load Input Image\n",
    "\n",
    "Upload your test image (e.g., `input.jpg`) to the Colab file system and update the image path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9d7d0-c39b-4b8b-8e08-2bde1b0a44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'input.jpg' with the path to your test image\n",
    "image_path = \"input.jpg\"\n",
    "frame = cv2.imread(image_path)\n",
    "if frame is None:\n",
    "    raise ValueError(\"Could not load the image. Please check the path.\")\n",
    "print(\"Image loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1b0e7-85e8-4c14-9a81-4c22c7877c36",
   "metadata": {},
   "source": [
    "## Depth Estimation with MiDaS\n",
    "\n",
    "Optionally, downscale the image for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0d46b-096d-4472-9c39-5e76e5a8a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_factor = 1.0  # Change to 0.5 (or another value) to downscale the image\n",
    "orig_height, orig_width, _ = frame.shape\n",
    "proc_width = int(orig_width * downscale_factor)\n",
    "proc_height = int(orig_height * downscale_factor)\n",
    "frame_proc = cv2.resize(frame, (proc_width, proc_height), interpolation=cv2.INTER_AREA)\n",
    "frame_rgb = cv2.cvtColor(frame_proc, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_batch = transform(frame_rgb).to(device)\n",
    "    prediction = midas(input_batch)\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=(proc_height, proc_width),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze()\n",
    "torch.cuda.synchronize()\n",
    "depth_map = prediction.cpu().detach().numpy()\n",
    "depth_map_norm = cv2.normalize(depth_map, None, 0, 1, norm_type=cv2.NORM_MINMAX)\n",
    "print(\"Depth estimation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e170a42-1467-47d4-a8a7-4a9b6f65a8da",
   "metadata": {},
   "source": [
    "## Create 3D Point Cloud using Open3D\n",
    "\n",
    "We create an RGB-D image from the color image and the estimated depth map, then generate a point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4b1a6-9451-44f3-a0cf-47df298ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d_color = o3d.geometry.Image(frame_rgb)\n",
    "o3d_depth = o3d.geometry.Image((depth_map_norm * 1000).astype(np.uint16))\n",
    "rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    o3d_color, o3d_depth,\n",
    "    depth_scale=1000.0,\n",
    "    convert_rgb_to_intensity=False\n",
    ")\n",
    "\n",
    "# Approximate camera intrinsics based on the processed image size\n",
    "fx = fy = proc_width  # Simplistic assumption\n",
    "ppx = proc_width / 2\n",
    "ppy = proc_height / 2\n",
    "intrinsic = o3d.camera.PinholeCameraIntrinsic(proc_width, proc_height, fx, fy, ppx, ppy)\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, intrinsic)\n",
    "\n",
    "# Adjust point cloud orientation\n",
    "pcd.transform([[1, 0, 0, 0],\n",
    "               [0, -1, 0, 0],\n",
    "               [0, 0, -1, 0],\n",
    "               [0, 0, 0, 1]])\n",
    "print(\"3D point cloud created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cdb0d-9ac0-4901-9a82-6a6a1c90b9af",
   "metadata": {},
   "source": [
    "## Pose Estimation and 3D Skeleton Creation\n",
    "\n",
    "We use MediaPipe to extract 2D pose landmarks from the image, then back-project them to 3D using the depth map. Adjust the maximum depth scaling as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3b5bd-b9ad-4b8b-92c0-64d9d0d2c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backproject_point(u, v, depth_value, fx, fy, ppx, ppy):\n",
    "    z = depth_value\n",
    "    x = (u - ppx) * z / fx\n",
    "    y = (v - ppy) * z / fy\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "max_depth_meters = 5.0  # Adjust if necessary\n",
    "results = pose_estimator.process(frame_rgb)\n",
    "keypoints_3d = []\n",
    "if results.pose_landmarks:\n",
    "    for landmark in results.pose_landmarks.landmark:\n",
    "        u = int(landmark.x * proc_width)\n",
    "        v = int(landmark.y * proc_height)\n",
    "        u_clamped = np.clip(u, 0, proc_width - 1)\n",
    "        v_clamped = np.clip(v, 0, proc_height - 1)\n",
    "        depth_val = depth_map_norm[v_clamped, u_clamped]\n",
    "        depth_in_meters = depth_val * max_depth_meters\n",
    "        keypoints_3d.append(backproject_point(u, v, depth_in_meters, fx, fy, ppx, ppy))\n",
    "else:\n",
    "    keypoints_3d = [np.array([0, 0, 0]) for _ in range(33)]\n",
    "keypoints_3d = np.array(keypoints_3d)\n",
    "print(\"3D skeleton (pose landmarks) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e3d6a5-b473-4e6f-8af6-26a4d0a982a3",
   "metadata": {},
   "source": [
    "## Visualize Point Cloud using Plotly\n",
    "\n",
    "This cell displays an interactive 3D scatter plot of the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917cfb2-38c5-46da-bc85-9c3b3c1e2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Open3D point cloud to NumPy arrays\n",
    "pts = np.asarray(pcd.points)\n",
    "if len(pcd.colors) > 0:\n",
    "    colors = np.asarray(pcd.colors)\n",
    "else:\n",
    "    colors = np.ones((pts.shape[0], 3))\n",
    "\n",
    "# Create a Plotly 3D scatter trace for the point cloud\n",
    "pcd_trace = go.Scatter3d(\n",
    "    x=pts[:, 0],\n",
    "    y=pts[:, 1],\n",
    "    z=pts[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        color=['rgb({},{},{})'.format(int(c[0]*255), int(c[1]*255), int(c[2]*255)) for c in colors],\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Point Cloud'\n",
    ")\n",
    "\n",
    "fig_pointcloud = go.Figure(data=[pcd_trace])\n",
    "fig_pointcloud.update_layout(scene=dict(aspectmode='data'),\n",
    "                             title=\"Point Cloud Only\")\n",
    "fig_pointcloud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dd153-278e-4d97-9d77-588e712a73ea",
   "metadata": {},
   "source": [
    "## Visualize Digital Twin (Point Cloud + Skeleton) using Plotly\n",
    "\n",
    "This cell overlays the 3D skeleton (pose landmarks) onto the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aaf3d7-5a30-4a48-b2e1-1d0c9a5a823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line traces for the skeleton\n",
    "line_traces = []\n",
    "for connection in pose_connections:\n",
    "    start_idx, end_idx = connection\n",
    "    if start_idx < len(keypoints_3d) and end_idx < len(keypoints_3d):\n",
    "        p0 = keypoints_3d[start_idx]\n",
    "        p1 = keypoints_3d[end_idx]\n",
    "        line_trace = go.Scatter3d(\n",
    "            x=[p0[0], p1[0]],\n",
    "            y=[p0[1], p1[1]],\n",
    "            z=[p0[2], p1[2]],\n",
    "            mode='lines',\n",
    "            line=dict(color='green', width=5),\n",
    "            showlegend=False\n",
    "        )\n",
    "        line_traces.append(line_trace)\n",
    "\n",
    "fig_digital_twin = go.Figure(data=[pcd_trace] + line_traces)\n",
    "fig_digital_twin.update_layout(scene=dict(aspectmode='data'),\n",
    "                              title=\"Digital Twin - 3D Model with Skeleton\")\n",
    "fig_digital_twin.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Digital_Twin_From_Image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
