{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58ea58d",
   "metadata": {},
   "source": [
    "# Digital Twin from Image using MiDaS, MediaPipe, and Plotly\n",
    "\n",
    "This notebook processes a single image (e.g., a frame extracted from a video) to create a 3D digital twin. It uses:\n",
    "\n",
    "- **MiDaS** for depth estimation\n",
    "- **MediaPipe** for pose (skeleton) detection\n",
    "- **Open3D** to build a point cloud from the RGBâ€‘D image\n",
    "- **Plotly** to visualize the 3D point cloud with an overlaid skeleton (as an interactive 3D scatter plot)\n",
    "\n",
    "The offscreen rendering of Open3D has been replaced by Plotly so that the notebook runs stably in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b12514",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y libosmesa6-dev libgl1-mesa-glx libglfw3\n",
    "\n",
    "!pip install opencv-python-headless mediapipe open3d torch torchvision plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import open3d as o3d\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Enable cuDNN benchmarking for optimal performance\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a5d4f",
   "metadata": {},
   "source": [
    "## Load MiDaS Model\n",
    "\n",
    "We use the small MiDaS model for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"MiDaS_small\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.small_transform if model_type == \"MiDaS_small\" else midas_transforms.default_transform\n",
    "print(\"MiDaS model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73bcb3",
   "metadata": {},
   "source": [
    "## Setup MediaPipe Pose\n",
    "\n",
    "We use MediaPipe in static image mode (since we process one image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose_estimator = mp_pose.Pose(static_image_mode=True,\n",
    "                              model_complexity=1,\n",
    "                              min_detection_confidence=0.5,\n",
    "                              min_tracking_confidence=0.5)\n",
    "pose_connections = mp_pose.POSE_CONNECTIONS\n",
    "print(\"MediaPipe Pose loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c26dd",
   "metadata": {},
   "source": [
    "## Load Input Image\n",
    "\n",
    "Upload your test image (e.g., `input.jpg`) to the Colab file system and update the image path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'input.jpg' with your image filename\n",
    "image_path = \"input.jpg\"\n",
    "frame = cv2.imread(image_path)\n",
    "if frame is None:\n",
    "    raise ValueError(\"Could not load the image. Please check the path.\")\n",
    "print(\"Image loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330a742",
   "metadata": {},
   "source": [
    "## Depth Estimation with MiDaS\n",
    "\n",
    "Optionally downscale the image for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_factor = 1.0  # Change to e.g. 0.5 to process at half resolution\n",
    "orig_height, orig_width, _ = frame.shape\n",
    "proc_width = int(orig_width * downscale_factor)\n",
    "proc_height = int(orig_height * downscale_factor)\n",
    "frame_proc = cv2.resize(frame, (proc_width, proc_height), interpolation=cv2.INTER_AREA)\n",
    "frame_rgb = cv2.cvtColor(frame_proc, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_batch = transform(frame_rgb).to(device)\n",
    "    prediction = midas(input_batch)\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=(proc_height, proc_width),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze()\n",
    "torch.cuda.synchronize()\n",
    "depth_map = prediction.cpu().detach().numpy()\n",
    "depth_map_norm = cv2.normalize(depth_map, None, 0, 1, norm_type=cv2.NORM_MINMAX)\n",
    "print(\"Depth estimation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67b041",
   "metadata": {},
   "source": [
    "## Create 3D Point Cloud using Open3D\n",
    "\n",
    "We create an RGBD image from the color image and the estimated depth map, then generate a point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d_color = o3d.geometry.Image(frame_rgb)\n",
    "o3d_depth = o3d.geometry.Image((depth_map_norm * 1000).astype(np.uint16))\n",
    "rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    o3d_color, o3d_depth,\n",
    "    depth_scale=1000.0,\n",
    "    convert_rgb_to_intensity=False\n",
    ")\n",
    "\n",
    "# Approximate camera intrinsics (based on processed resolution)\n",
    "fx = fy = proc_width  # Simplistic assumption\n",
    "ppx = proc_width / 2\n",
    "ppy = proc_height / 2\n",
    "intrinsic = o3d.camera.PinholeCameraIntrinsic(proc_width, proc_height, fx, fy, ppx, ppy)\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, intrinsic)\n",
    "\n",
    "# Adjust point cloud orientation (flip axes as needed)\n",
    "pcd.transform([[1, 0, 0, 0],\n",
    "               [0, -1, 0, 0],\n",
    "               [0, 0, -1, 0],\n",
    "               [0, 0, 0, 1]])\n",
    "print(\"3D point cloud created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14de8c5",
   "metadata": {},
   "source": [
    "## Pose Estimation and 3D Skeleton Creation\n",
    "\n",
    "We use MediaPipe to extract 2D pose landmarks and then back-project them to 3D using the depth map. Adjust the assumed maximum depth as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3007c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backproject_point(u, v, depth_value, fx, fy, ppx, ppy):\n",
    "    z = depth_value\n",
    "    x = (u - ppx) * z / fx\n",
    "    y = (v - ppy) * z / fy\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "max_depth_meters = 5.0  # Adjust if needed\n",
    "results = pose_estimator.process(frame_rgb)\n",
    "keypoints_3d = []\n",
    "if results.pose_landmarks:\n",
    "    for landmark in results.pose_landmarks.landmark:\n",
    "        u = int(landmark.x * proc_width)\n",
    "        v = int(landmark.y * proc_height)\n",
    "        u_clamped = np.clip(u, 0, proc_width - 1)\n",
    "        v_clamped = np.clip(v, 0, proc_height - 1)\n",
    "        depth_val = depth_map_norm[v_clamped, u_clamped]\n",
    "        depth_in_meters = depth_val * max_depth_meters\n",
    "        keypoints_3d.append(backproject_point(u, v, depth_in_meters, fx, fy, ppx, ppy))\n",
    "else:\n",
    "    keypoints_3d = [np.array([0, 0, 0]) for _ in range(33)]\n",
    "keypoints_3d = np.array(keypoints_3d)\n",
    "print(\"3D skeleton (pose landmarks) created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f95e46",
   "metadata": {},
   "source": [
    "## Visualize 3D Digital Twin using Plotly\n",
    "\n",
    "We convert the Open3D point cloud to a NumPy array and create a 3D scatter plot. Then we overlay the skeleton as line traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert point cloud to NumPy array\n",
    "pts = np.asarray(pcd.points)\n",
    "if len(pcd.colors) > 0:\n",
    "    colors = np.asarray(pcd.colors)\n",
    "else:\n",
    "    colors = np.ones((pts.shape[0], 3))\n",
    "\n",
    "# Create a Plotly 3D scatter trace for the point cloud\n",
    "pcd_trace = go.Scatter3d(\n",
    "    x=pts[:, 0],\n",
    "    y=pts[:, 1],\n",
    "    z=pts[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        color=['rgb({},{},{})'.format(int(c[0]*255), int(c[1]*255), int(c[2]*255)) for c in colors],\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Point Cloud'\n",
    ")\n",
    "\n",
    "# Create line traces for the skeleton\n",
    "line_traces = []\n",
    "for connection in pose_connections:\n",
    "    start_idx, end_idx = connection\n",
    "    if start_idx < len(keypoints_3d) and end_idx < len(keypoints_3d):\n",
    "        p0 = keypoints_3d[start_idx]\n",
    "        p1 = keypoints_3d[end_idx]\n",
    "        line_trace = go.Scatter3d(\n",
    "            x=[p0[0], p1[0]],\n",
    "            y=[p0[1], p1[1]],\n",
    "            z=[p0[2], p1[2]],\n",
    "            mode='lines',\n",
    "            line=dict(color='green', width=5),\n",
    "            showlegend=False\n",
    "        )\n",
    "        line_traces.append(line_trace)\n",
    "\n",
    "fig = go.Figure(data=[pcd_trace] + line_traces)\n",
    "fig.update_layout(scene=dict(aspectmode='data'),\n",
    "                  title=\"Digital Twin - 3D Model from Image\")\n",
    "fig.show()\n",
    "\n",
    "# Optionally, you can save the figure as an HTML file\n",
    "# fig.write_html(\"digital_twin.html\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Digital_Twin_From_Image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
